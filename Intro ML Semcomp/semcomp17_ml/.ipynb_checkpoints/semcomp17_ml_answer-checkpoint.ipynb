{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (ML)\n",
    "This tutorial aims to get you familiar with the basis of ML. You will go through several tasks to build some basic regression and classification models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#essential imports\n",
    "import sys\n",
    "sys.path.insert(1,'utils')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib nbagg\n",
    "import pandas as pd\n",
    "import ml_utils\n",
    "print np.__version__\n",
    "print np.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "### 1. 1. Univariate linear regression\n",
    "Let start with the most simple regression example. Firstly, read the data in a file named \"house_price_statcrunch.xls\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data = pd.ExcelFile('data/house_price_statcrunch.xls').parse(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see what is inside by printing out the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \" \".join([field.ljust(10) for field in house_data.keys()])\n",
    "for i in xrange(10):\n",
    "    print \" \".join([str(house_data[field][i]).ljust(10) for field in house_data.keys()])\n",
    "TOTALS = len(house_data['House'])\n",
    "print \"...\\n\\nTotal number of samples: {}\".format(TOTALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let preserve some data for test. Here we extract 10% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.random.permutation(TOTALS)\n",
    "idx_train = idx[:90]\n",
    "idx_test = idx[90:]\n",
    "house_data_train = {}\n",
    "house_data_test = {}\n",
    "for field in house_data.keys():\n",
    "    house_data_test[field] = house_data[field][idx_test]\n",
    "    house_data_train[field] = house_data[field][idx_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For univariate regression, we are interested in the \"size\" parameter only. Let's extract necessary data and visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Z = ml_utils.extract_data(house_data, ['size'], ['price'])\n",
    "Z = Z/1000.0  #price has unit x1000 USD\n",
    "\n",
    "plt.plot(X[0],Z[0], '.')\n",
    "plt.xlabel('size (feet^2)')\n",
    "plt.ylabel('price (USD x1000)')\n",
    "plt.title('house data scatter plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to build a house price prediction model that will approximate the price of a house given its size. To do it, we need to fit a linear line (y = ax + b) to the data above using linear regression. Remember the procedure:\n",
    "1. Define training set\n",
    "2. Define hypothesis function. Here $F(x,W) = Wx$\n",
    "3. Loss function. Here $L(W) = \\frac{1}{2N}{\\sum_{i=1}^N{(F(x^{(i)},W)-z)^2}}$\n",
    "4. Update procedure (gradient descent). $W = W - k\\frac{\\partial L}{\\partial W}$\n",
    "\n",
    "To speed up computation, you should avoid using loop when working with scripting languges e.g. Python, Matlab. Try using array/matrix instead. Here you are provided code for step 1 and 2. Your will be asked to implement step 3 and 4. Some skeleton code will be provided for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 1: define training and test set X, Z.\"\"\"\n",
    "X_train, Z_train = ml_utils.extract_data(house_data_train, ['size'], ['price'])\n",
    "X_test, Z_test = ml_utils.extract_data(house_data_test, ['size'], ['price'])\n",
    "\n",
    "Z_train = Z_train/1000.0                        #price has unit x1000 USD\n",
    "Z_test = Z_test/1000.0\n",
    "\n",
    "##normalise data, uncomment for now\n",
    "#X_train, u, scale = ml_utils.normalise_data(X_train)\n",
    "#X_test = ml_utils.normalise_data(X_test, u, scale)\n",
    "\n",
    "N = Z_train.size                                            #number of training samples\n",
    "ones_array = np.ones((1,N),dtype=np.float32)\n",
    "X_train = np.concatenate((X_train, ones_array), axis=0)           #why?\n",
    "X_test  = np.concatenate((X_test, np.ones((1, Z_test.size), dtype=np.float32)), axis = 0) #same for test data\n",
    "print \"size of X_train \", X_train.shape\n",
    "print \"size of Z_train \", Z_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 2: define hypothesis function\"\"\"\n",
    "def F_Regression(X, W):\n",
    "    \"\"\"\n",
    "    Compute the hypothesis function y=F(x,W) in batch.\n",
    "    input:  X  input array, must has size DxN (each column is one sample)\n",
    "            W  parameter array, must has size 1xD\n",
    "    output: linear multiplication of W*X, size 1xN\n",
    "    \"\"\"\n",
    "    return np.dot(W,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: define the loss function for linear regression according to the following formula:\n",
    "$$L = \\frac{1}{2N}{\\sum_{i=1}^N{(y^{(i)}-z^{(i)})^2}}$$\n",
    "Please fill in the skeleton code below. Hints: (i) in Python numpy the square operator $x^2$ is implemented as x**2; (ii) try to use matrix form and avoid for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 3: loss function\"\"\"\n",
    "def Loss_Regression(Y, Z):\n",
    "    \"\"\"\n",
    "    Compute the loss between the predicted (Y=F(X,W)) and the groundtruth (Z) values.\n",
    "    input:   Y   predicted results Y = F(X,W) with given parameter W, has size 1xN\n",
    "             Z   groundtruth vector Z, has size 1xN\n",
    "    output:  loss value, is a scalar\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    diff = Y-Z\n",
    "    return 1/(2*N)*np.dot(diff, diff.T).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: compute gradient of the loss function w.r.t parameter W according to the following formula:<br>\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{1}{N}\\sum_{i=1}^N{(y^{(i)}-z^{(i)})x^{(i)}}$$\n",
    "Please fill in the skeleton code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"step 4: gradient descent - compute gradient\"\"\"\n",
    "def dLdW_Regression(X, Y, Z):\n",
    "    \"\"\"\n",
    "    Compute gradient of the loss w.r.t parameter W.\n",
    "    input:   X   input array, each column is one sample, has size DxN\n",
    "             Y   predicted values, has size 1xN\n",
    "             Z   groundtruth values, has size 1xN\n",
    "    output:  gradient, has same size as W\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    return 1/N * (Y-Z).dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform gradient descent update procedure according to the following formula:\n",
    "$$W = W - k\\frac{\\partial L}{\\partial W}$$\n",
    "Here we use fixed number of iterations and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"step 4: gradient descent - update loop\"\"\"\n",
    "np.random.seed(0)\n",
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 1e-8                                #learning rate\n",
    "niters = 160                            #number of training iterations\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = niters/50\n",
    "loss_collections = []\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "axis_data = fig.add_subplot(132)\n",
    "for i in xrange(niters):\n",
    "    Y_train = F_Regression(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Regression(Y_train, Z_train)         #compute loss\n",
    "    dLdW = dLdW_Regression(X_train, Y_train, Z_train) #compute gradient\n",
    "    W = W - k*dLdW                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if (i+1)% vis_interval == 0:\n",
    "        ml_utils.plot_loss(axis_loss, range(i+1),loss_collections, \"loss = \" + str(loss))\n",
    "        ml_utils.plot_scatter_and_line(axis_data, X_train, Z_train, W, \"iter #\" + str(i))\n",
    "        fig.canvas.draw()\n",
    "print \"Learned parameters \", W.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your learned model using the test set. Measure the total error of your prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = F_Regression(X_test, W)\n",
    "error = Loss_Regression(Y_test, Z_test)\n",
    "print \"Evaluation error: \", error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz**: you may notice the learning rate k is set to $10^{-8}$. Why is it too small? Try to play with several bigger values of k, you will soon find out that the training is extremely sensitive to the learning rate (the training easily  diverges or even causes \"overflow\" error with large k).<br><br>\n",
    "Answer: It is because both the input (size of house) and output (price) have very large range of values, which result in very large gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: Test your learned model. Suppose you want to sell a house of size 3000 $feat^2$, how much do you expect your house will cost?<br>\n",
    "Answer: you should get around 260k USD for that house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3000\n",
    "x = np.array([x,1])[...,None]   #make sure feature vector has size 2xN, here N=1\n",
    "print \"Expected price: \", F_Regression(x,W).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4**: The gradient descent in the code above terminates after 100 iterations. You may want it to terminate when improvement in the loss is below a threshold.\n",
    "$$\\Delta L_t = |L_t - L_{t-1}| < \\epsilon$$\n",
    "Edit the code to terminate the loop when the loss improvement is below $\\epsilon=10^{-2}$. Re-evaluate your model to see if its performance has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 4: gradient descent - update loop\"\"\"\n",
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 1e-8                                #learning rate\n",
    "epsilon = 1e-2                            #terminate condition\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = 10\n",
    "loss_collections = []\n",
    "prev_loss = 0\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "axis_data = fig.add_subplot(132)\n",
    "while(1):\n",
    "    Y_train = F_Regression(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Regression(Y_train, Z_train)         #compute loss\n",
    "    dLdW = dLdW_Regression(X_train, Y_train, Z_train) #compute gradient\n",
    "    W = W - k*dLdW                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if abs(loss - prev_loss) < epsilon:\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    \n",
    "    if (len(loss_collections)+1) % vis_interval==0:\n",
    "        #print \"Iter #\", len(loss_collections)\n",
    "        ml_utils.plot_loss(axis_loss, range(len(loss_collections)),loss_collections, \"loss = \" + str(loss))\n",
    "        ml_utils.plot_scatter_and_line(axis_data, X_train, Z_train, W, \"iter #\" + str(len(loss_collections)))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "print \"Learned parameters \", W.squeeze()\n",
    "print \"Learning terminates after {} iterations\".format(len(loss_collections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the test\n",
    "Y_test = F_Regression(X_test, W)\n",
    "error = Loss_Regression(Y_test, Z_test)\n",
    "print \"Evaluation error: \", error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the error measurement on the test set has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multivariate regression\n",
    "So far we assume the house price is affected by the size only. Now let consider also other fields \"Bedrooms\", \"Baths\", \"lot\" (location) and \"NW\" (whether or not the houses face Nothern West direction).<br><br>\n",
    "**Important**: now your feature vector is multi-dimensional, it is crucial to normalise your training set for gradient descent to converge properly. The code below is almost identical to the previous step 1, except it loads more fields and implements data normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 1: define training set X, Z.\"\"\"\n",
    "selected_fields = ['size', 'Bedrooms', 'Baths', 'lot', 'NW']\n",
    "X_train, Z_train = ml_utils.extract_data(house_data_train, selected_fields, ['price'])\n",
    "X_test, Z_test = ml_utils.extract_data(house_data_test, selected_fields, ['price'])\n",
    "\n",
    "Z_train = Z_train/1000.0                        #price has unit x1000 USD\n",
    "Z_test = Z_test/1000.0\n",
    "\n",
    "##normalise \n",
    "X_train, u, scale = ml_utils.normalise_data(X_train)\n",
    "X_test = ml_utils.normalise_data(X_test, u, scale)\n",
    "\n",
    "N = Z_train.size                                            #number of training samples\n",
    "ones_array = np.ones((1,N),dtype=np.float32)\n",
    "X_train = np.concatenate((X_train, ones_array), axis=0)           #why?\n",
    "X_test  = np.concatenate((X_test, np.ones((1, Z_test.size), dtype=np.float32)), axis = 0) #same for test data\n",
    "print \"size of X_train \", X_train.shape\n",
    "print \"size of Z_train \", Z_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run step 2-4 again. Note the followings: \n",
    "1. You need not to modify the *Loss_Regression* and *dLdW_Regression* functions. They should generalise enough to work with multi-dimensional data\n",
    "2. Since your training samples are normalised you can now use much higher learning rate e.g. k = 1e-2\n",
    "3. Note that the plot function *plot_scatter_and_line* will not work in multivariate regression since it is designed for 1-D input only. Consider commenting it out.<br>\n",
    "\n",
    "**Question**: how many iterations are required to pass the threshold $\\Delta L < 10^{-2}$ ?<br>\n",
    "Answer: ~4000 iterations (and it will take a while to complete).\n",
    "\n",
    "**Task 1.5**: (a) evaluate your learned model on the test set. (b) Suppose the house you want to sell has a size of 3000 $feet^2$, has 3 bedrooms, 2 baths, lot number 10000 and in NW direction. How much do you think its price would be? Hints: don't forget to normalise the test sample.<br>\n",
    "\n",
    "Answer: You will get ~150k USD only, much lower than the previous prediction based on size only. Your house has an advantage of size, but other parameters matter too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 4: gradient descent - update loop\"\"\"\n",
    "\"\"\" same code but change k = 1e-2\"\"\"\n",
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 1e-2                                #learning rate\n",
    "epsilon = 1e-2                            #terminate condition\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = 10\n",
    "loss_collections = []\n",
    "prev_loss = 0\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "#axis_data = fig.add_subplot(132)\n",
    "while(1):\n",
    "    Y_train = F_Regression(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Regression(Y_train, Z_train)         #compute loss\n",
    "    dLdW = dLdW_Regression(X_train, Y_train, Z_train) #compute gradient\n",
    "    W = W - k*dLdW                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if abs(loss - prev_loss) < epsilon:\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    \n",
    "    if (len(loss_collections)+1) % vis_interval==0:\n",
    "        #print \"Iter #\", len(loss_collections)\n",
    "        ml_utils.plot_loss(axis_loss, range(len(loss_collections)),loss_collections, \"loss = \" + str(loss))\n",
    "        #ml_utils.plot_scatter_and_line(axis_data, X_train, Z_train, W, \"iter #\" + str(len(loss_collections)))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "print \"Learned parameters \", W.squeeze()\n",
    "print \"Learning terminates after {} iterations\".format(len(loss_collections))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"apply on the test set\"\"\"\n",
    "Y_test = F_Regression(X_test, W)\n",
    "error = Loss_Regression(Y_test, Z_test)\n",
    "print \"Evaluation error: \", error\n",
    "\n",
    "\"\"\"test a single sample\"\"\"\n",
    "x = np.array([3000, 3,2, 10000, 1],dtype=np.float32)[...,None]\n",
    "x = ml_utils.normalise_data(x, u, scale)\n",
    "x = np.concatenate((x,np.ones((1,1))),axis=0)\n",
    "print \"Price: \", F_Regression(x,W).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gradient descent with momentum\n",
    "In the latest experiment, our training takes ~4000 iterations to converge. Now let try gradient descent with momentum to speed up the training. We will employ the following formula:\n",
    "$$v_t = m*v_{t-1} + k\\frac{\\partial L}{\\partial W}$$\n",
    "$$W = W - v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"step 4: gradient descent with momentum - update loop\"\"\"\n",
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 1e-2                                #learning rate\n",
    "epsilon = 1e-2                            #terminate condition\n",
    "m = 0.9                                 #momentum\n",
    "v = 0                                   #initial velocity\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = 10\n",
    "loss_collections = []\n",
    "prev_loss = 0\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "#axis_data = fig.add_subplot(132)\n",
    "while(1):\n",
    "    Y_train = F_Regression(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Regression(Y_train, Z_train)         #compute loss\n",
    "    dLdW = dLdW_Regression(X_train, Y_train, Z_train) #compute gradient\n",
    "    v = v*m + k*dLdW\n",
    "    W = W - v                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if abs(loss - prev_loss) < epsilon:\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    if (len(loss_collections)+1) % vis_interval==0:\n",
    "        #print \"Iter #\", len(loss_collections)\n",
    "        ml_utils.plot_loss(axis_loss, range(len(loss_collections)),loss_collections, \"loss = \" + str(loss))\n",
    "        #ml_utils.plot_scatter_and_line(axis_data, X_train, Z_train, W, \"iter #\" + str(len(loss_collections)))\n",
    "        fig.canvas.draw()\n",
    "\n",
    "print \"Learned parameters \", W.squeeze()\n",
    "print \"Learning terminates after {} iterations\".format(len(loss_collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "In this part you will walk through different steps to implement several basic classification tasks.\n",
    "### 2.1. Binary classification\n",
    "Imagine you were an USP professor who teaches Computer Science. This year there is 100 year-one students who want to register your module. You examine their performance based on their scores on two exams. You have gone through the records of 80 students and already made admission decisions for them. Now you want to build a model to automatically make admission decisions for the rest 20 students. Your training data will be the exam results and admission decisions for the 80 students that you have assessed.<br><br>\n",
    "Firstly, let load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = pd.read_csv('data/student_data_binary_clas.txt', header = None, names=['exam1', 'exam2', 'decision'])\n",
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train/test set\n",
    "X = np.array([student_data['exam1'], student_data['exam2']], dtype=np.float32)\n",
    "Z = np.array([student_data['decision']], dtype = np.float32)\n",
    "\n",
    "#assume the first 80 students have been assessed, use them as the training data\n",
    "X_train = X[:,:80]\n",
    "X_test = X[:,80:]\n",
    "\n",
    "#you later have to manually assess the rest 20 students according to the university policies.\n",
    "# Great, now you have a chance to evaluate your learned model\n",
    "Z_train = Z[:,:80]\n",
    "Z_test = Z[:,80:]\n",
    "\n",
    "#normalise data\n",
    "X_train, u, scale = ml_utils.normalise_data(X_train)\n",
    "X_test = ml_utils.normalise_data(X_test, u, scale)\n",
    "\n",
    "#concatenate array of \"1s\" to X array\n",
    "X_train = np.concatenate((X_train, np.ones_like(Z_train)), axis = 0)\n",
    "X_test = np.concatenate((X_test, np.ones_like(Z_test)), axis = 0)\n",
    "\n",
    "#let visualise the training set\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1)\n",
    "axis_data = fig.add_subplot(111)\n",
    "ml_utils.plot_scatter_with_label_2d(axis_data, X_train, Z_train,msg=\"student score scatter plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**: your first task is to define the hypothesis function. Do you remember the hypothesis function in a binary classification task? It has form of a sigmoid function:\n",
    "$$F(x,W) = \\frac{1}{1+e^{-Wx}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_Classification(X, W):\n",
    "    \"\"\"\n",
    "    Compute the hypothesis function given input array X and parameter W\n",
    "    input:   X  input array, must has size DxN (each column is one sample)\n",
    "             W  parameter array, must has size 1xD\n",
    "    output:  sigmoid of W*X, size 1xN\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-np.dot(W,X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: define the loss function for binary classification. It is called \"negative log loss\":\n",
    "$$L(W) = -\\frac{1}{N} \\sum_{i=1}^N{[z^{(i)} log(F(x^{(i)},W)) + (1-z^{(i)})(log(1-F(x^{(i)},W))]}$$\n",
    "\n",
    "Next, define the gradient function:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{1}{N}(F(X,W) - Z)X^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"step 3: loss function for classification\"\"\"\n",
    "def Loss_Classification(Y, Z):\n",
    "    \"\"\"\n",
    "    Compute the loss between the predicted (Y=F(X,W)) and the groundtruth (Z) values.\n",
    "    input:   Y   predicted results Y = F(X,W) with given parameter W, has size 1xN\n",
    "             Z   groundtruth vector Z, has size 1xN\n",
    "    output:  loss value, is a scalar\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    \n",
    "    return -1/N*(np.dot(np.log(Y), Z.T) + np.dot(np.log(1-Y), (1-Z).T)).squeeze()\n",
    "\n",
    "\"\"\"step 4: gradient descent for classification - compute gradient\"\"\"\n",
    "def dLdW_Classification(X, Y, Z):\n",
    "    \"\"\"\n",
    "    Compute gradient of the loss w.r.t parameter W.\n",
    "    input:   X   input array, each column is one sample, has size DxN\n",
    "             Y   probability of label = 1, has size 1xN\n",
    "             Z   groundtruth values, has size 1xN\n",
    "    output:  gradient, has same size as W\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    return 1/N * (Y-Z).dot(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 0.2                                #learning rate\n",
    "epsilon = 1e-6                            #terminate condition\n",
    "m = 0.9                                 #momentum\n",
    "v = 0                                   #initial velocity\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = 10\n",
    "loss_collections = []\n",
    "prev_loss = 0\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "axis_data = fig.add_subplot(132)\n",
    "while(1):\n",
    "    Y_train = F_Classification(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Classification(Y_train, Z_train)         #compute loss\n",
    "    dLdW = dLdW_Classification(X_train, Y_train, Z_train) #compute gradient\n",
    "    v = v*m + k*dLdW\n",
    "    W = W - v                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if abs(loss - prev_loss) < epsilon:\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    if (len(loss_collections)+1) % vis_interval==0:\n",
    "        ml_utils.plot_loss(axis_loss, range(len(loss_collections)),loss_collections, \"loss = \" + str(loss))\n",
    "        ml_utils.plot_scatter_with_label_2d(axis_data, X_train, Z_train, W, \"student score scatter plot\")\n",
    "        fig.canvas.draw()\n",
    "\n",
    "print \"Learned parameters \", W.squeeze()\n",
    "print \"Learning terminates after {} iterations\".format(len(loss_collections))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "Y_test = F_Classification(X_test, W)\n",
    "predictions = Y_test > 0.5\n",
    "accuracy = np.sum(predictions == Z_test)/float(Z_test.size)\n",
    "print \"Test accuracy: \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve 90% accuracy (only two students have been misclassified). Not too bad, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**: regularisation\n",
    "Now we want to add a regularisation term into the loss to prevent overfitting.\n",
    "Regularisation loss is simply magnitude of the parameter vector W after removing the last element (i.e. bias doesn't count to regularisation).\n",
    "$$L_R = \\frac{1}{2}|W'|^2$$\n",
    "where W' is W with the last element truncated.<br>\n",
    "\n",
    "Now the total loss would be:\n",
    "$$L(W) = -\\frac{1}{N} \\sum_{i=1}^N{[z^{(i)} log(F(x^{(i)},W)) + (1-z^{(i)})(log(1-F(x^{(i)},W))]} + \\frac{1}{2}|W'|^2$$\n",
    "\n",
    "The gradient become:\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{1}{N}(F(X,W) - Z)X^T + W''$$\n",
    "where W'' is W with the last element change to 0.\n",
    "\n",
    "Your task is to implement the loss and gradient function with added regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"step 3: loss function with regularisation\"\"\"\n",
    "def Loss_Classification_Reg(Y, Z, W):\n",
    "    \"\"\"\n",
    "    Compute the loss between the predicted (Y=F(X,W)) and the groundtruth (Z) values.\n",
    "    input:   Y   predicted results Y = F(X,W) with given parameter W, has size 1xN\n",
    "             Z   groundtruth vector Z, has size 1xN\n",
    "             W   parameter vector, size 1xD\n",
    "    output:  loss value, is a scalar\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    W_ = W[:,:-1]\n",
    "    return -1/N*(np.dot(np.log(Y), Z.T) + np.dot(np.log(1-Y), (1-Z).T)).squeeze() + 0.5*np.dot(W_,W_.T).squeeze()\n",
    "\n",
    "\"\"\"step 4: gradient descent with regularisation - compute gradient\"\"\"\n",
    "def dLdW_Classification_Reg(X, Y, Z, W):\n",
    "    \"\"\"\n",
    "    Compute gradient of the loss w.r.t parameter W.\n",
    "    input:   X   input array, each column is one sample, has size DxN\n",
    "             Y   probability of label = 1, has size 1xN\n",
    "             Z   groundtruth values, has size 1xN\n",
    "             W   parameter vector, size 1xD\n",
    "    output:  gradient, has same size as W\n",
    "    \"\"\"\n",
    "    #enter the code here\n",
    "    N = float(Z.size)\n",
    "    W_ = W\n",
    "    W_[:,-1] = 0\n",
    "    return 1/N * (Y-Z).dot(X.T) + W_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the update loop again with the new loss and gradient functions. Note you may need to change the learning rate accordingly to have proper convergence. Now you have implemented both regularisation and momentum techniques, you can use a standard learning rate value of 0.01 which is widely used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gradient descent with regularisation- parameter update loop\"\"\"\n",
    "W = np.random.rand(1,X_train.shape[0]).astype(np.float32)   #W has size 1xD, randomly initialised\n",
    "k = 0.01                                #learning rate\n",
    "epsilon = 1e-6                            #terminate condition\n",
    "m = 0.9                                 #momentum\n",
    "v = 0                                   #initial velocity\n",
    "\n",
    "#visualisation settings\n",
    "vis_interval = 10\n",
    "loss_collections = []\n",
    "prev_loss = 0\n",
    "plt.close()\n",
    "plt.ion()\n",
    "fig = plt.figure(1,figsize=(16, 4))\n",
    "axis_loss = fig.add_subplot(131)\n",
    "axis_data = fig.add_subplot(132)\n",
    "for i in range(500):\n",
    "    Y_train = F_Classification(X_train,W)               #compute hypothesis function aka. predicted values\n",
    "    loss = Loss_Classification_Reg(Y_train, Z_train, W)         #compute loss\n",
    "    dLdW = dLdW_Classification_Reg(X_train, Y_train, Z_train, W) #compute gradient\n",
    "    v = v*m + k*dLdW\n",
    "    W = W - v                            #update\n",
    "    loss_collections.append(loss)\n",
    "    if abs(loss - prev_loss) < epsilon:\n",
    "        break\n",
    "    prev_loss = loss\n",
    "    if (len(loss_collections)+1) % vis_interval==0:\n",
    "        ml_utils.plot_loss(axis_loss, range(len(loss_collections)),loss_collections, \"loss = \" + str(loss))\n",
    "        ml_utils.plot_scatter_with_label_2d(axis_data, X_train, Z_train, W, \"student score scatter plot\")\n",
    "        fig.canvas.draw()\n",
    "\n",
    "print \"Learned parameters \", W.squeeze()\n",
    "print \"Learning terminates after {} iterations\".format(len(loss_collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you see any improvement in accuracy or convergence speed? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Regularisation does help speed up the training (it adds stricter rules to the update procedure). Accuracy is the same (90%) is probably because (i) number of parameters to be trained is small (2-D) and so is the number of training samples; and (ii) the data are well separated. In a learning task which involves large number of parameters (such as neural network), regularisation proves a very efficient technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-class classification\n",
    "Here we are working with a very famous dataset. The Iris flower dataset has 150 samples of 3 Iris flower species (Setosa, Versicolour, and Virginica), each sample stores the height and length of its sepal and pedal in cm (4-D in total). Your task is to build a classifier to distinguish these flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the Iris dataset\n",
    "iris = np.load('data/iris.npz')\n",
    "X = iris['X']\n",
    "Z = iris['Z']\n",
    "print \"size X \", X.shape\n",
    "print \"size Z \", Z.shape\n",
    "\n",
    "#split train/test with ratio 120:30\n",
    "TOTALS = Z.size\n",
    "idx = np.random.permutation(TOTALS)\n",
    "idx_train = idx[:120]\n",
    "idx_test = idx[120:]\n",
    "\n",
    "X_train = X[:, idx_train]\n",
    "X_test = X[:, idx_test]\n",
    "Z_train = Z[:, idx_train]\n",
    "Z_test = Z[:, idx_test]\n",
    "\n",
    "#normalise data\n",
    "X_train, u, scale = ml_utils.normalise_data(X_train)\n",
    "X_test = ml_utils.normalise_data(X_test, u, scale)\n",
    "\n",
    "#concatenate array of \"1s\" to X array\n",
    "X_train = np.concatenate((X_train, np.ones_like(Z_train)), axis = 0)\n",
    "X_test = np.concatenate((X_test, np.ones_like(Z_test)), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4**: one-vs-all. Train 3 binary one-vs-all classifiers $F_i$ (i=1-3), one for each class. An unknown feture vector x belongs to class i if:\n",
    "$$max_i F(x,W_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5**: implement one-vs-one and compare the results with one-vs-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
