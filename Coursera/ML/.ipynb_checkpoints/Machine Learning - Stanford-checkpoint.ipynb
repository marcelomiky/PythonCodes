{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning definitions:\n",
    "\n",
    "Arthur Samuel (1959). ML: Field of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "Tom Mitchell (1998). Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some tak T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: Suppose your email program watches which emails you do or do nor mark as spam, and based on that learns how to better filter spam. \n",
    "\n",
    "- The task T is: Classifying emails as spam or not spam\n",
    "- The experience E is: Watching you label emails as spam or not spam\n",
    "- The performance measure P: the number (or fraction) of emails correctly classified as spam/not spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: playing checkers.\n",
    "\n",
    "- E = the experience of playing many games of checkers\n",
    "- T = the task of playing checkers.\n",
    "- P = the probability that the program will win the next game.\n",
    "\n",
    "In general, any machine learning problem can be assigned to one of two broad classifications:\n",
    "\n",
    "Supervised learning and Unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\n",
    "\n",
    "Supervised learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.\n",
    "\n",
    "We could turn this example into a classification problem by instead making our output about whether the house \"sells for more or less than the asking price.\" Here we are classifying the houses based on price into two discrete categories.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture\n",
    "\n",
    "(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cocktail party problem\n",
    "\n",
    "There's a party, room full of people, all sitting around, all talking at the same time and there are all these overlapping voices because everyone is talking at the same time, and it is almost hard to hear the person in front of you. \n",
    "\n",
    "So maybe at a cocktail party with two people, two people talking at the same time, and it's a somewhat small cocktail party. And we're going to put two microphones in the room so there are microphones, and because these microphones are at two different distances from the speakers, each microphone records a different combination of these two speaker voices. \n",
    "\n",
    "Maybe speaker one is a little louder in microphone one and maybe speaker two is a little bit louder on microphone 2 because the 2 microphones are at different positions relative to the 2 speakers, but each microphone would cause an **overlapping combination of both speakers' voices**. \n",
    "\n",
    "So here's an actual recording of two speakers recorded by a researcher. Let me play for you the first, what the first microphone sounds like. \n",
    "\n",
    "*One (uno), two (dos), three (tres), four (cuatro), five (cinco), six (seis), seven (siete), eight (ocho), nine (nueve), ten (y diez).*\n",
    "\n",
    "All right, maybe not the most interesting cocktail party, there's two people counting from one to ten in two languages but you know. What you just heard was the first microphone recording, here's the second recording. \n",
    "\n",
    "*Uno (one), dos (two), tres (three), cuatro (four), cinco (five), seis (six), siete (seven), ocho (eight), nueve (nine) y diez (ten).*\n",
    "\n",
    "So we can do, is take these two microphone recorders and give them to an Unsupervised Learning algorithm called the cocktail party algorithm, and tell the algorithm - find structure in this data for you. And what the algorithm will do is listen to these audio recordings and say, you know it sounds like the two audio recordings are being added together or that have being summed together to produce these recordings that we had. Moreover, what the cocktail party algorithm will do is separate out these two audio sources that were being added or being summed together to form other recordings and, in fact, here's the first output of the cocktail party algorithm. \n",
    "\n",
    "*One, two, three, four, five, six, seven, eight, nine, ten.*\n",
    "\n",
    "So, I separated out the English voice in one of the recordings. And here's the second of it. \n",
    "\n",
    "*Uno, dos, tres, quatro, cinco, seis, siete, ocho, nueve y diez.* \n",
    "\n",
    "Not too bad, to give you one more example, here's another recording of another similar situation, here's the first microphone: \n",
    "\n",
    "*One, two, three, four, five, six, seven, eight, nine, ten.*\n",
    "\n",
    "OK so the poor guy's gone home from the cocktail party and he's now sitting in a room by himself talking to his radio. Here's the second microphone recording. \n",
    "\n",
    "*One, two, three, four, five, six, seven, eight, nine, ten.*\n",
    "\n",
    "When you give these two microphone recordings to the same algorithm, what it does, is again say, you know, it sounds like there are two audio sources, and moreover, the album says, here is the first of the audio sources I found. \n",
    "\n",
    "*One, two, three, four, five, six, seven, eight, nine, ten.*\n",
    "\n",
    "So that wasn't perfect, it got the voice, but it also got a little bit of the music in there. Then here's the second output to the algorithm. \n",
    "\n",
    "Not too bad, in that second output it managed to get rid of the voice entirely. And just, you know, cleaned up the music, got rid of the counting from one to ten. \n",
    "\n",
    "So you might look at an Unsupervised Learning algorithm like this and ask how complicated this is to implement this. It seems like in order to, you know, build this application, it seems like to do this audio processing you need to write a ton of code or maybe link into like a bunch of synthesizer Java libraries that process audio, seems like a really complicated program, to do this audio, separating out audio and so on. \n",
    "\n",
    "**It turns out the algorithm, to do what you just heard, that can be done with one line of code - shown right here.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It take researchers a long time to come up with this line of code. I'm not saying this is an easy problem, But it turns out that when you use the right programming environment, many learning algorithms can be really short programs.\n",
    "\n",
    "So this is also why in this class we're going to use the Octave programming environment. \n",
    "\n",
    "Octave, is free open source software, and using a tool like Octave or Matlab, many learning algorithms become just a few lines of code to implement. Later in this class, I'll just teach you a little bit about how to use Octave and you'll be implementing some of these algorithms in Octave. Or if you have Matlab you can use that too. \n",
    "\n",
    "It turns out the Silicon Valley, for a lot of machine learning algorithms, what we do is first prototype our software in Octave because software in Octave makes it incredibly fast to implement these learning algorithms. \n",
    "\n",
    "Here each of these functions like for example the SVD function that stands for singular value decomposition; but that turns out to be a linear algebra routine, that is just built into Octave. \n",
    "\n",
    "If you were trying to do this in C++ or Java, this would be many many lines of code linking complex C++ or Java libraries. So, you can implement this stuff as C++ or Java or Python, it's just much more complicated to do so in those languages. \n",
    "\n",
    "**What I've seen after having taught machine learning for almost a decade now, is that, you learn much faster if you use Octave as your programming environment, and if you use Octave as your learning tool and as your prototyping tool, it'll let you learn and prototype learning algorithms much more quickly.**\n",
    "\n",
    "And in fact what many people will do to in the large Silicon Valley companies is in fact, use an algorithm like Octave to first prototype the learning algorithm, and only after you've gotten it to work, then you migrate it to C++ or Java or whatever. It turns out that by doing things this way, you can often get your algorithm to work much faster than if you were starting out in C++. \n",
    "\n",
    "So, I know that as an instructor, I get to say \"trust me on this one\" only a finite number of times, but for those of you who've never used these Octave type programming environments before, I am going to ask you to trust me on this one, and say that you, you will, I think your time, your development time is one of the most valuable resources. \n",
    "\n",
    "And having seen lots of people do this, I think you as a machine learning researcher, or machine learning developer will be much more productive if you learn to start in prototype, to start in Octave, in some other language. \n",
    "\n",
    "Finally, to wrap up this video, I have one quick review question for you. \n",
    "\n",
    "We talked about Unsupervised Learning, which is a learning setting where you give the algorithm a ton of data and just ask it to find structure in the data for us.\n",
    "\n",
    "For example: \n",
    "\n",
    "- Given a set of news articles found on the web, group them into sets of articles about the same stories.\n",
    "- Given a database of customers data, automatically discover market segments and groups customers into different market segments.\n",
    "\n",
    "So, hopefully, you've remembered the spam folder problem. If you have labeled data, you know, with spam and non-spam e-mail, we'd treat this as a Supervised Learning problem. \n",
    "\n",
    "The news story example, that's exactly the Google News example that we saw in this video, we saw how you can use a clustering algorithm to cluster these articles together so that's Unsupervised Learning. \n",
    "\n",
    "The market segmentation example I talked a little bit earlier, you can do that as an Unsupervised Learning problem because I am just gonna get my algorithm data and ask it to discover market segments automatically. \n",
    "\n",
    "And the final example, diabetes, well, that's actually just like our breast cancer example from the last video. Only instead of, you know, good and bad cancer tumors or benign or malignant tumors we instead have diabetes or not and so we will use that as a supervised, we will solve that as a Supervised Learning problem just like we did for the breast tumor data. \n",
    "\n",
    "So, that's it for Unsupervised Learning and in the next video, we'll delve more into specific learning algorithms and start to talk about just how these algorithms work and how we can, how you can go about implementing them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.\n",
    "\n",
    "We can derive this structure by clustering the data based on relationships among the variables in the data.\n",
    "\n",
    "With unsupervised learning there is no feedback based on the prediction results.\n",
    "\n",
    "Example:\n",
    "\n",
    "Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n",
    "\n",
    "Non-clustering: The \"Cocktail Party Algorithm\", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    1. A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. What would be a reasonable choice for P?\n",
    "\n",
    "The probability of it correctly predicting a future date's weather.\n",
    "\n",
    "    2. Suppose you are working on weather prediction, and your weather station makes one of three predictions for each day's weather: Sunny, Cloudy or Rainy. You'd like to use a learning algorithm to predict tomorrow's weather. Would you treat this as a classification or a regression problem?\n",
    "\n",
    "Classification\n",
    "\n",
    "    3. Suppose you are working on stock market prediction, and you would like to predict the price of a particular stock tomorrow (measured in dollars). You want to use a learning algorithm for this.Would you treat this as a classification or a regression problem?\n",
    "\n",
    "Regression\n",
    "\n",
    "    4. (NOT RIGHT) Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case, assume some appropriate dataset is available for your algorithm to learn from.\n",
    "\n",
    "Examine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.\n",
    "\n",
    "Given 50 articles written by male authors, and 50 articles written by female authors, learn to predict the gender of a new manuscript's author (when the identity of this author is unknown).\n",
    "\n",
    "Given historical data of children's ages and heights, predict children's height as a function of their age.\n",
    "\n",
    "    5. Which of these is a reasonable definition of machine learning?\n",
    "\n",
    "Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Linear regression.\n",
    "\n",
    "Data set of housing prices from the city of Portland, Oregon, and plot the data set of a number of houses that were different sizes that were sold for a range of different prices. \n",
    "\n",
    "Let's say that given this data set, you have a friend that's trying to sell a house and let's see if friend's house is size of 1250 square feet and you want to tell them how much they might be able to sell the house for. \n",
    "\n",
    "Well one thing you could do is fit a model. Maybe fit a straight line to this data. Looks something like that and based on that, maybe you could tell your friend that let's say maybe he can sell the house for around $220,000. \n",
    "\n",
    "So this is an example of a supervised learning algorithm. And it's supervised learning because we're given the, quotes, \"right answer\" for each of our examples. Namely we're told what was the actual house, what was the actual price of each of the houses in our data set were sold for and moreover, this is an example of a **regression problem** where the term regression refers to the fact that we are predicting a real-valued output namely the price. \n",
    "\n",
    "And just to remind you the other most common type of supervised learning problem is called the classification problem where we predict discrete-valued outputs such as if we are looking at cancer tumors and trying to decide if a tumor is malignant or benign. So that's a zero-one valued discrete output. \n",
    "\n",
    "More formally, in supervised learning, we have a data set and this data set is called a training set. So for housing prices example, we have a training set of different housing prices and our job is to learn from this data how to predict prices of the houses. \n",
    "\n",
    "Let's define some notation that we're using throughout this course. We're going to define quite a lot of symbols. \n",
    "\n",
    "- m = Number of training examples \n",
    "- x's = 'input' variable/feature\n",
    "- y's = 'output' variable/feature\n",
    "- (x,y) = one training example\n",
    "- $(x^{(i)}, y^{(i)})$ = ith training example (i here is not exponentiation! It is an **index**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"mlfigure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this data set, if I have, you know, let's say 47 rows in this table. Then I have 47 training examples and m equals 47. Let me use lowercase x to denote the input variables often also called the features. That would be the x is here, it would the input features. And I'm gonna use y to denote my output variables or the target variable which I'm going to predict and so that's the second column here. \n",
    "\n",
    "(x, y) to denote a single training example. So, a single row in this table (ex: 2104, 460) corresponds to a single training example and to refer to a specific training example.\n",
    "\n",
    "x(i),y(i) to refer to the ith training example. \n",
    "\n",
    "So for example: \n",
    "\n",
    "$x^{(1)} = 2104$\n",
    "\n",
    "$x^{(2)} = 1416$\n",
    "\n",
    "$y^{(1)} = 460$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"mlfigure2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's how this supervised learning algorithm works. We saw that with the training set like our training set of housing prices and we feed that to our learning algorithm. Is the job of a learning algorithm to then output a function which by convention is usually denoted lowercase h and h stands for hypothesis.\n",
    "\n",
    "And what the job of the hypothesis is, is, is a function that takes as input the size of a house like maybe the size of the new house your friend's trying to sell so it takes in the value of x and it tries to output the estimated value of y for the corresponding house. \n",
    "\n",
    "So h is a function that maps from x's to y's. \n",
    "\n",
    "**In machine learning, hypotesis is a name that was used in the early days of machine learning and it kinda stuck. 'Cause maybe not a great name for this sort of function, for mapping from sizes of houses to the predictions, that you know.... I think the term hypothesis, maybe isn't the best possible name for this, but this is the standard terminology that people use in machine learning. So don't worry too much about why people call it that. When designing a learning algorithm, the next thing we need to decide is how do we represent this hypothesis h.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm going to choose our initial choice , for **representing the hypothesis**, will be the following. We're going to represent h as follows. \n",
    "\n",
    "And we will write this as \n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x$\n",
    "\n",
    "And as a shorthand, sometimes instead of writing: $h(x)$\n",
    "\n",
    "<img src=\"mlfigure3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cost Function - Intuition I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In linear progression, we have a training set that I showed here remember on notation M was the number of training examples, so maybe m equals 47. And the form of our hypothesis, which we use to make predictions is this linear function. \n",
    "\n",
    "To introduce a little bit more terminology, these theta zero and theta one, they stabilize what I call the parameters of the model. And what we're going to do in this video is talk about how to go about choosing these two parameter values, theta 0 and theta 1. \n",
    "\n",
    "<img src=\"mlfigure4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With different choices of the parameter's theta 0 and theta 1, we get different hypothesis, different hypothesis functions. I know some of you will probably be already familiar with what I am going to do on the slide, but just for review, here are a few examples. If theta 0 is 1.5 and theta 1 is 0, then the hypothesis function will look like this. \n",
    "\n",
    "Because your hypothesis function will be h of x equals 1.5 plus 0 times x which is this constant value function which is phat at 1.5. If theta0 = 0, theta1 = 0.5, then the hypothesis will look like this, and it should pass through this point 2,1 so that you now have h(x). Or really h of theta(x), but sometimes I'll just omit theta for brevity. So h(x) will be equal to just 0.5 times x, which looks like that. And finally, if theta zero equals one, and theta one equals 0.5, then we end up with a hypothesis that looks like this. Let's see, it should pass through the two-two point. \n",
    "\n",
    "<img src=\"mlfigure5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In linear regression, we have a training set, like maybe the one I've plotted here. What we want to do, is come up with values for the parameters theta zero and theta one so that the straight line we get out of this, corresponds to a straight line that somehow fits the data well, like maybe that line over there. So, how do we come up with values, theta zero, theta one, that corresponds to a good fit to the data? \n",
    "\n",
    "The idea is we get to choose our parameters theta 0, theta 1 so that h of x, meaning the value we predict on input x, that this is at least close to the values y for the examples in our training set, for our training examples. \n",
    "\n",
    "So in our training set, we've given a number of examples where we know X decides the wholes and we know the actual price is was sold for. So, let's try to choose values for the parameters so that, at least in the training set, given the X in the training set we make reason of the active predictions for the Y values. Let's formalize this. \n",
    "\n",
    "So linear regression, what we're going to do is, I'm going to want to solve a **minimization problem**. So I'll write minimize over theta0 theta1. And I want this to be small. **I want the difference between h(x) and y to be small.** \n",
    "\n",
    "And one thing I might do is try to minimize the square difference between the output of the hypothesis and the actual price of a house. Okay. So lets find some details. \n",
    "\n",
    "You remember that I was using the notation (x(i),y(i)) to represent the ith training example. So what I want really is to sum over my training set, something i = 1 to m, of the square difference between, this is the prediction of my hypothesis when it is input to size of house number i. Minus the actual price that house number I was sold for, and I want to minimize the sum of my training set, sum from I equals one through M, of the difference of this squared error, the square difference between the predicted price of a house, and the price that it was actually sold for. \n",
    "\n",
    "And just remind you of notation, **m** here was the size of my training set. So **m** there is my number of training examples. Right that hash sign is the abbreviation for number of training examples. And to make some of our, make the math a little bit easier, I'm going to actually look at we are 1 over m times that so let's try to minimize my average minimize one over 2m. Putting the 2 at the constant one half in front, it may just sound the math probably easier so minimizing one-half of something, right, should give you the same values of the process, theta 0 theta 1, as minimizing that function. \n",
    "\n",
    "$J(\\theta_0,\\theta_1) =  \\frac{1}{2m} \\sum_{i=1}^{m}(ŷ_i - y_i)^2 =  \\frac{1}{2m} \\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2$\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That is equal to this plus theta one xi. And this notation, minimize over theta 0 theta 1, this means you'll find me the values of theta 0 and theta 1 that causes this expression to be minimized and this expression depends on theta 0 and theta 1, okay? \n",
    "\n",
    "So just a recap. We're closing this problem as, find me the values of theta zero and theta one so that the average, the 1 over the 2m, times the sum of square errors between my predictions on the training set minus the actual values of the houses on the **training set is minimized**. So this is going to be my overall objective function for linear regression. \n",
    "\n",
    "Cost function = $J(\\theta_0,\\theta_1)$\n",
    "\n",
    "And what I want to do is **minimize over theta0 and theta1**. My function j(theta0, theta1). Just write this out. \n",
    "This is my cost function. So, this cost function is also called the **squared error function**. \n",
    "\n",
    "When sometimes called the squared error cost function and it turns out that why do we take the squares of the erros. It turns out that these **squared error cost function is a reasonable choice and works well for problems for most regression programs. There are other cost functions that will work pretty well. But the square cost function is probably the most commonly used one for regression problems.** \n",
    "\n",
    "So far we've just seen a mathematical definition of this cost function. In case this function j of theta zero, theta one. In case this function seems a little bit abstract, and you still don't have a good sense of what it's doing, in the next video, in the next couple videos, I'm actually going to go a little bit deeper into what the cause function \"J\" is doing and try to give you better intuition about what is computing and why we want to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To recap, here's what we had last time. We want to fit a straight line to our data, so we had this formed as a hypothesis with these parameters theta zero and theta one, and with different choices of the parameters we end up with different straight line: \n",
    "\n",
    "<img src=\"mlfigure6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In pictures what this means is that if theta zero equals zero that corresponds to choosing only hypothesis functions that pass through the origin, that pass through the point (0, 0). Using this simplified definition of a hypothesizing cost function let's try to understand the cost function concept better. \n",
    "\n",
    "It turns out that two key functions we want to understand. The first is the hypothesis function, and the second is a cost function. So, notice that the hypothesis, right, H of X. For a face value of theta one, this is a function of X. So the hypothesis is a function of, what is the size of the house X. In contrast, the cost function, J, that's a function of the parameter, theta one, which controls the slope of the straight line. Let's plot these functions and try to understand them both better. Let's start with the hypothesis. On the left, let's say here's my training set with three points at (1, 1), (2, 2), and (3, 3). Let's pick a value theta one, so when theta one equals one, and if that's my choice for theta one, then my hypothesis is going to look like this straight line over here. And I'm gonna point out, when I'm plotting my hypothesis function. X-axis, my horizontal axis is labeled X, is labeled you know, size of the house over here. Now, of temporary, set theta one equals one, what I want to do is figure out what is J(theta1), when theta one equals one. \n",
    "\n",
    "So let's go ahead and compute what the cost function has for. You'll devalue one. \n",
    "\n",
    "<img src=\"mlfigure7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mlfigure8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$J_{(\\theta_1)} = J_{(0.5)} = 0.58$. O que isso significa?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para $\\theta_1 = 0$\n",
    "\n",
    "$J_{(0)} = \\frac{1}{2m}[(1 - 0)^2 + (2 - 0)^2 + (3 - 0)^2] = \\frac{1}{6}[1 + 4 + 9] = \\frac{14}{6}$\n",
    "\n",
    "<img src=\"mlfigure09.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**For each value of theta one corresponds to a different hypothesis, or to a different straight line fit on the left. And for each value of theta one, we could then derive a different value of j of theta one.**\n",
    "\n",
    "- $\\theta_1 = 1$ (ver gráfico à direita) corresponde a reta azul claro. \n",
    "- $\\theta_1 = 0.5$ (ver gráfico à direita) corresponde a reta magenta. \n",
    "- $\\theta_1 = 0$ (ver gráfico à direita) corresponde a reta azul escura. \n",
    "\n",
    "** We want to choose the value of theta one that minimizes J of theta one. This was our objective function for the linear regression. Looking at this curve, the value that minimizes j of theta one is theta one equals to one. **\n",
    "\n",
    "Thus as a goal, we should try to minimize the cost function. In this case, θ1=1 is our global minimum.\n",
    "\n",
    "And low and behold, that is indeed the best possible straight line fit through our data, by setting theta one equals one. And just, for this particular training set, we actually end up fitting it perfectly. And that's why minimizing j of theta one corresponds to finding a straight line that fits the data well. \n",
    "\n",
    "So, to wrap up. In this video, we looked up some plots. To understand the cost function. To do so, we simplify the algorithm. So that it only had one parameter theta one. And we set the parameter theta zero to be only zero. In the next video. We'll go back to the original problem formulation and look at some visualizations involving both theta zero and theta one. That is without setting theta zero to zero. And hopefully that will give you, an even better sense of what the cost function j is doing in the original linear regression formulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cost Function - Intuition II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This video assumes that you're familiar with contour plots. \n",
    "\n",
    "Here's our problem formulation as usual, with the hypothesis parameters, cost function, and our optimization objective. \n",
    "\n",
    "\n",
    "Hypothesis: $h_\\theta(x) = \\theta_0 + \\theta_1x$\n",
    "\n",
    "Parameters: $\\theta_0, \\theta_1$\n",
    "\n",
    "Cost Function: $J(\\theta_0,\\theta_1) =  \\frac{1}{2m} \\sum_{i=1}^{m}(ŷ_i - y_i)^2 =  \\frac{1}{2m} \\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2$\n",
    "\n",
    "Goal: $\\begin{matrix}minimize\\\\\\theta_0, \\theta_1\\end{matrix}J(\\theta_0,\\theta_1)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm going to keep both of my parameters, theta zero, and theta one, as we generate our visualizations for the cost function. \n",
    "\n",
    "So, same as last time, we want to understand the hypothesis H and the cost function J. \n",
    "\n",
    "So, here's my training set of housing prices and let's make some hypothesis. \n",
    "\n",
    "This is not a particularly good hypothesis. \n",
    "\n",
    "But, if I set theta zero=50 and theta one=0.06, then I end up with this hypothesis down here and that corresponds to that straight line. \n",
    "\n",
    "<img src=mlfigure11.png> \n",
    "\n",
    "<img src=mlfigure11_1.png>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now given these value of theta zero and theta one, we want to plot the corresponding cost function on the right. \n",
    "\n",
    "What we did last time was when we **only had theta one**. \n",
    "\n",
    "In other words, drawing plots that look like this as a function of theta one. \n",
    "\n",
    "***But now we have two parameters, theta zero, and theta one, and so the plot gets a little more complicated.***\n",
    "\n",
    "It turns out that when we have only one parameter, that the parts we drew had this sort of bow shaped function. \n",
    "\n",
    "Now, when we have two parameters, it turns out the cost function also has a similar sort of bow shape. And, in \n",
    "fact, depending on your training set, you might get a cost function that maybe looks something like this. \n",
    "\n",
    "So, this is a 3-D surface plot, where the axes are labeled theta zero and theta one. \n",
    "\n",
    "<img src=mlfigure3d.png>\n",
    "\n",
    "So as you **vary theta zero and theta one, the two parameters, you get different values of the cost function J (theta zero, theta one)** and the **height of this surface above a particular point of theta zero, theta one.**\n",
    "\n",
    "Right, that's the vertical axis. ***The height of the surface of the points indicates the value of $J(\\theta_0,\\theta_1$).*** And you can see it sort of has this bow like shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of illustration in the rest of this video I'm not actually going to use these sort of 3D surfaces to show you the cost function J, instead I'm going to use **contour plots**:\n",
    "\n",
    "<img src=contourplots.png>\n",
    "\n",
    "Or what I also call contour figures. I guess they mean the same thing. To show you these surfaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's an example of a contour figure, shown on the right, where the axis are theta zero and theta one. And what each of these ovals, what each of these ellipsis shows is a set of points that takes on the same value for J(theta zero, theta one). So concretely, for example this, you'll take that point and that point and that point. ***All three of these points that I just drew in magenta, they have the same value for $J(\\theta_0,\\theta_1)$.*** \n",
    "\n",
    "This is the theta zero, theta one axis but ***those three have the same Value for $J(\\theta_0,\\theta_1)$*** and if you haven't seen contour plots much before think of, imagine if you will. *A bow shaped function that's coming out of my screen.* (CONTOUR PLOTS! ;D) \n",
    "\n",
    "<img src =mlcostfunction1.png>\n",
    "\n",
    "So that the minimum, so the bottom of the bow is this point right there, right? This middle, the middle of these concentric ellipses. And imagine a bow shape that sort of grows out of my screen like this, so that each of these ellipses, you know, has the same height above my screen. And the minimum with the bow, right, is right down there. And so the contour figures is a, is way to, is maybe a more convenient way to visualize my function J. \n",
    "\n",
    "So, let's look at some examples. Over here, I have a particular point, right? And so this is, with, you know, theta zero equals maybe about 800, and theta one equals maybe a -0.15 . And so this point, right, this point corresponds to one set of pair values of theta zero, theta one and the corresponding, in fact, to that hypothesis, right, theta zero is about 800, that is, where it intersects the vertical axis is around 800, and this is slope of about -0.15. \n",
    "\n",
    "<img src =mlcostfunction2.png>\n",
    "\n",
    "Now this line is really not such a good fit to the data, right. This hypothesis, h(x), with these values of theta zero, theta one, it's really not such a good fit to the data. And so you find that, it's cost. **Is a value that's out here that's you know pretty far from the minimum right it's pretty far this is a pretty high cost because this is just not that good a fit to the data.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some more examples. Now here's a different hypothesis that's you know still not a great fit for the data but may be slightly better so here right that's my point that those are my parameters theta zero theta one and so my theta zero value. That's about 360 and my value for theta one is equal to zero. Let's take:\n",
    "\n",
    "- $\\theta_0 = 360$\n",
    "- $\\theta_1 = 0$\n",
    "\n",
    "And this pair of parameters corresponds to that hypothesis, corresponds to flat line, that is, \n",
    "\n",
    "$h_{(x)} = 360 + 0.x$\n",
    "\n",
    "So that's the hypothesis. And this hypothesis again has some cost, and that cost is, you know, plotted as the **height of the J function** at that point. \n",
    "\n",
    "<img src=mlcostfunction3.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at just a couple of examples. Here's one more, you know, at this value of theta zero, and at that value of theta one, we end up with this hypothesis, h(x) and again, not a great fit to the data, and is actually further away from the minimum. Last example, this is actually not quite at the minimum, but it's pretty close to the minimum. So this is not such a bad fit to the, to the data, where, for a particular value, of, theta zero. Which, one of them has value, as in for a particular value for theta one. We get a particular h(x). And this is, this is not quite at the minimum, but it's pretty close. And so the sum of squares errors is sum of squares distances between my, training samples and my hypothesis. Really, that's a sum of square distances, right? Of all of these errors. This is pretty close to the minimum even though it's not quite the minimum. So with these figures I hope that gives you a better understanding of what values of the cost function J, how they are and how that corresponds to different hypothesis and so as how better hypotheses may corresponds to points that are closer to the minimum of this cost function J. Now of course what we really want is an efficient algorithm, right, a efficient piece of software for automatically finding The value of theta zero and theta one, that minimizes the cost function J, right? And what we, what we don't wanna do is to, you know, how to write software, to plot out this point, and then try to manually read off the numbers, that this is not a good way to do it. And, in fact, we'll see it later, that when we look at more complicated examples, we'll have high dimensional figures with more parameters, that, it turns out, we'll see in a few, we'll see later in this course, examples where this figure, you know, cannot really be plotted, and this becomes much harder to visualize. And so, what we want is to have software to find the value of theta zero, theta one that minimizes this function and in the next video we start to talk about an algorithm for automatically finding that value of theta zero and theta one that minimizes the cost function J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below.\n",
    "\n",
    "<img src=\"mlpicture1.png\">\n",
    "\n",
    "Taking any color and going along the 'circle', one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(\\theta_0, \\theta_1)$ and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $\\theta_0$ = 800 and $\\theta_1$= -0.15. Taking another h(x) and plotting its contour plot, one gets the following graphs:\n",
    "\n",
    "<img src=\"mlpicture2.png\">\n",
    "\n",
    "When $\\theta_0$ = 360 and $\\theta_1$ = 0, the value of  $J(\\theta_0, \\theta_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.\n",
    "\n",
    "<img src=\"mlpicture3.png\">\n",
    "\n",
    "The graph above minimizes the cost function as much as possible and consequently, the result of $\\theta_1$ and $\\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most 'circle'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sk]",
   "language": "python",
   "name": "conda-env-sk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
